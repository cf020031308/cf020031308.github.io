Definition: [$] span(v_1, ..., v_m) = \{a_1v_1 + ... + a_mv_m: a_1, ..., a_m âˆˆ F\} [/$]
%
Definition [$$]\begin{align}
  & (v_1, ..., v_m) \text{ is linearly independent}
  & \iff \sum_{j=1}^m a_jv_j = 0 \text{ implies } a_j = 0
[/$$]
%
Note: {{c1::Empty list}} is defined to be linearly independent and it spans {{c2::{0}}}.
%
Linear Dependence Lemma [$$]\begin{align}
  (v_1 \neq 0, ..., v_m) \text{ is linearly dependent, then } \\
  2 \leq \exists j \leq m \text{ that } \begin{cases}
    v_j \in span(v_1, ..., v_{j-1}) \\
    span(v_1, ..., v_{j-1}, v_{j+1}, ..., v_m) = span(v_1, ..., v_m)
  \end{cases}
\end{align}[/$$]
%
Definition: A vector space is called {{c1::finite dimensional}} if {{c2::some list of vectors in it spans the space}}.
%
Theorem: In a {{c1::finite-dimensional}} vector space, the length of every linearly independent list of vectors is {{c2::less than or equal to}} the length of every spanning list of vectors.
%
Proposition: Every subspaces of a finite-dimensional vector space is {{c1::finite dimensional}}.
%
Definition: A {{c1::basis}} of V is a list of vectors in V that is {{c2::linearly independent and spans V}}.
%
Proposition: [$$]\begin{align}
  & (v_1, ..., v_n) \text{ is a basis of } V \\
  & \iff \forall v \in V, \text{ uniquely } \exists a_j \in F, \text{ that } \sum_{j=1}^n a_jv_j = v
\end{align}[/$$]
%
Theorem: Every spanning list in a vector space can be reduced to a basis of the vector space.
%
Corollary: Every finite-dimensional vector space has a basis.
%
Theorem: Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.
%
Proposition: Suppose V is finite dimensional and U is a subspace of V. Then there is a subspace W of V such that V = U âŠ• W.

## Dimension

- **Theorem**: Any two bases of a finite-dimensional vector space have the same length.
- **Definition**: The ***dimension*** of a finite-dimensional vector space V is defined to be the length of any basis of the vector space, denoted dimV.
- **Proposition**: If V is finite dimensional and U is a subspace of V, then dimU â‰¤ dimV.
- **Proposition**: If V is finite dimensional, then every spanning list of vectors in V with length dimV is a basis of V.
- **Proposition**: If V is finite dimensional, then every linearly independent list of vectors in V with length dimV is a basis of V.
- **Theorem**: If U and W are subspaces of a finite-dimensional vector space, then dim(U + W) = dimU + dimW - dim(U âˆ© W).
- **Proposition**: Suppose V is finite dimensional and U1, ..., Um are subspaces of V such that V = U1 + ... + Um and dimV = dimU1 + ... + dimUm. Then V = U1 âŠ• ... âŠ• Um.

# 3. Linear Maps

## Definitions and Examples

### Linear Map

A ***linear map*** from V to W is a function T: V -> W with the following properties:

- **additivity**: T(u + v) = Tu + Tv for all u, v âˆˆ V;
- **homogeneity**: T(av) = a(Tv) for all a âˆˆ F and all v âˆˆ V.

Examples:

- **zero**: 0 âˆˆ ğ“›(V, W) and 0v = 0.
- **identity**: I âˆˆ ğ“›(V, V) and Iv = v
- **differentiation**: T âˆˆ ğ“›(ğ“Ÿ(R), ğ“Ÿ(R)) and Tp = p'
- **integration**: T âˆˆ ğ“›(ğ“Ÿ(R), R) and Tp = âˆ«(0->1)p(x)dx
- **multiplication by** x^2: T âˆˆ ğ“›(ğ“Ÿ(R), ğ“Ÿ(R)) and (Tp)x = xxp(x)
- **backward shift**: T âˆˆ ğ“›(F^âˆ, F^âˆ) and T(x1, x2, x3, ...) = (x2, x3, ...)
- **from F^n to F^m**: T âˆˆ ğ“›(F^n, F^m) and T(x1, ..., xn) = (a11x1 + ... + a1nxn, ..., am1x1 + ... + amnxn)

### Linear Map as a Vector Space

The set of all linear maps from V to W is denoted ğ“›(V, W).

By defining addition (S + T)v = Sv + Tv for v âˆˆ V and scalar multiplication (aT)v = a(Tv) for v âˆˆ V, ğ“›(V, W) is a vector space.

As you should also verify:

1. S + T âˆˆ ğ“›(V, W)
2. aT âˆˆ ğ“›(V, W)

### Product

The ***product*** of S âˆˆ ğ“›(V, W) and T âˆˆ ğ“›(U, V) denoted ST is defined as (ST)(v) = S(Tv), ST âˆˆ ğ“›(U, W).
And it has most of the usual properties expected of a product:

- **associativity**: (T1T2)T3 = T1(T2T3)
- **identity**: TI = IT = T
- **distributive properties**: (S1 + S2)T = S1T + S2T and S(T1 + T2) = ST1 + ST2

Raise a counterexample of commutativity.

## Null Spaces and Ranges

- **Definition**: For T âˆˆ ğ“›(V, W), the ***null space*** of T, denoted nullT, is the subset of V consisting of those vectors that T maps to 0: nullT = {v âˆˆ V: Tv = 0}.
- **Proposition**: If T âˆˆ ğ“›(V, W), then nullT is a subspace of V.
- **Definition**: A linear map T: V -> W is called ***injective*** if whenever u, v âˆˆ V and Tu = Tv, we have u = v.
- **Proposition**: Let T âˆˆ ğ“›(V, W). Then V is injective if and only if nullT = {0}.
- **Definition**: For T âˆˆ ğ“›(V, W), the ***range*** of T, denoted rangeT, is the subset of W consisting of those vectors that are of the form Tv for some v âˆˆ V: rangeT = {Tv: v âˆˆ V}.
- **Proposition**: If T âˆˆ ğ“›(V, W), then rangeT is a subspace of W.
- **Definition**: A linear map T: V -> W is called ***surjective*** if its range equals W.
- **Theorem**: If V is finite dimensional and T âˆˆ ğ“›(V, W), then rangeT is a finite-dimensional subspace of W and dimV = dim nullT + dim rangeT.
- **Corollary**: If V and W are finite-dimensional vector spaces such that dimV > dimW, then no linear map from V to W is injective.
- **Corollary**: If V and W are finite-dimensional vector spaces such that dimV `<` dimW, then no linear map from V to W is surjective.

## The Matrix of a Linear Map

- **Definition**: An m-by-n ***matrix*** is a rectangular array with m rows and n columns.
- **Definition**: ***The matrix of a linear map*** T with respect to the bases (v1, ..., vn) of V and (w1, ..., wm) of W, denoted M(T, (v1, ..., vn), (w1, ..., wm)), or just M(T) if the bases are clear from the context, is the m-by-n matrix formed by the a's if Tvk = a(1, k)w1 + ... + a(m, k)wm for each k = 1, ..., n.
- **Note**: With addition of matrices of the same size defined by adding corresponding entries in the matrices, M(T + S) = M(T) + M(S).
- **Note**: With product of a scalar and a matrix defined by multiplying each entry in the matrix by the scalar, M(cT) = cM(T).
- **Note**: With addtition and scalar multiplication defined as above, the set of all m-by-n matrices with entries in F, denoted Mat(m, n, F) is a vector space.
- **Note**: To make M(TS) = M(T)M(S), define matrix multiplication of m-by-n matrix A and n-by-p matrix B, denoted AB, to be the m-by-p matrix whose entry in row j, column k equals âˆ‘(r=1,n)a(j, r)b(r, k).
- **Definition**: ***The matrix of a vector*** v, denoted M(v), is the n-by-1 matrix of b's if v = b1v1 + ... + bnvn where (v1, ..., vn) is a basis of V.
- **Proposition**: Suppose T âˆˆ ğ“›(V, W) and (v1, ..., vn) is a basis of V and (w1, ..., wm) is a basis of M. Then M(Tv) = M(T)M(v) for every v âˆˆ V.

## Invertibility

- **Definition**: If T âˆˆ ğ“›(V, W) is ***invertible***, then the ***inverse*** of T, denoted (T^-1), is the unique element of ğ“›(W,V) such that (T^âˆ’1)T = I and T(T^âˆ’1) = I.
- **Proposition**: A linear map is invertible if and only if it is injective and surjective.
- **Definition**: Two vector spaces are called ***isomorphic*** if there is an invertible linear map from one vector space onto the other one.
- **Theorem**: Two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension.
- **Proposition**: Suppose that dimV = n and dimW = m. Then M is an invertible linear map between ğ“›(V, W) and Mat(m, n, F).
- **Proposition**: If V and W are finite dimensional, then ğ“›(V, W) is finite dimensional and dim ğ“›(V, W) = (dim V)(dim W).
- **Definition**: A linear map from a vector space to itself, denoted T: V -> V or T âˆˆ ğ“›(V), is called an ***operator***.
- **Thereom**: Suppose V is finite dimensional. If T âˆˆ ğ“›(V), then the following are equivalent:
    - T is invertible;
    - T is injective;
    - T is surjective.

# 4. Polynomials

- **Proposition**: Suppose p âˆˆ ğ“Ÿ(F) is a polynomial with degree m >= 1. Let â„· âˆˆ F. Then â„· is a root of p if and only if there is a polynomial q âˆˆ ğ“Ÿ(F) with degree m - 1 such that p(z) = (z - â„·)q(z) for all z âˆˆ F.
- **Corollary**: Suppose p âˆˆ ğ“Ÿ(F) is a polynomial with degree m >= 0. Then p has at most m distinct roots in F.
- **Corollary**: Suppose a0, ..., am âˆˆ F. If a0 + a1z + a2z^2... + amz^m = 0 for all z âˆˆ F, then a0 = ... = am = 0.
- **Definition**: The degree of a polynomial is unique, denoted ***deg p***.
- **Division Algorithm**: Suppose p, q âˆˆ ğ“Ÿ(F), with p â‰  0. Then there exist polynomials s, r âˆˆ ğ“Ÿ(F) such that q = sp + r and `deg r < deg p`.
- **Fundamental Theorem of Algebra**: Every nonconstant polynomial with complex coefficients has a root. (As you could not verify yet.)
- **Corollary**: if p âˆˆ ğ“Ÿ(F) is a nonconstant polynomial, then p has a unique factorization (except for the order of the factors) of the form p(z) = c(z - â„·1)...(z - â„·m) where c, â„·1, ..., â„·m âˆˆ C.
- **Proposition**: Suppose p is a polynomial with real coefficients. If â„· âˆˆ C is a root of p, then so is the ***complex conjugate*** of â„·. (And these two factors appear the same number of times if they are not equal.)
- **Proposition**: Let ğ›‚, ğ›ƒ âˆˆ R. Then there is a polynomial factorization of the form x^2 + ğ›‚x + ğ›ƒ = (x - â„·1)(x - â„·2) with â„·1, â„·2 âˆˆ R, if and only if ğ›‚^2 >= 4ğ›ƒ.
- **Theorem**: If p âˆˆ ğ“Ÿ(R) is a nonconstant polynomial, then p has a unique factorization (except for the order of the factors) of the form p(x) = c(x - â„·1)...(x - â„·m)(x^2 + ğ›‚1x + ğ›ƒ1)...(x^2 + ğ›‚nx + ğ›ƒn), where c, â„·1, ..., â„·m âˆˆ R and (ğ›‚1, ğ›ƒ1), ..., (ğ›‚n, ğ›ƒn) âˆˆ R^2 with `ğ›‚j^2 < 4ğ›ƒj` for each j.

# 5. Eigenvalues and Eigenvectors

## Invariant Subspaces

- **Definition**: For T âˆˆ ğ“›(V) and U a subspace of V, we say the U is ***invariant*** under T if u âˆˆ U implies Tu âˆˆ U.
- **Definition**: A scalar â„· âˆˆ F is called an ***eigenvalue*** of T âˆˆ ğ“›(V) if there exists a nonzero vector u âˆˆ V such that Tu = â„·u.
- **Note**: â„· is an eigenvalue of T if and only if T - â„·I is not invertible/injective/surjective.
- **Definition**: Suppose T âˆˆ ğ“›(V) and â„· âˆˆ F is an eigenvalue of T. A vector u âˆˆ V is called an ***eigenvector*** of T (corresponding to â„·) if Tu = â„·u.
- **Note**: The set of eigenvectors of T corresponding to â„· equals null(T - â„·I).
- **Theorem**: Let T âˆˆ ğ“›(V). Suppose â„·1, ..., â„·m are distinct eigenvalues of T and v1, ..., vm are corresponding nonzero eigenvectors. Then (v1, ..., vm) is linearly independent.
- **Corollary**: Each operator on V has at most dimV distinct eigenvalues.

## Polynomials Applied to Operators

## Upper-Triangular Matrices

- **Theorem**: Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.
- **Note**: A central goal of linear algebra is to show that given an operator T âˆˆ ğ“›(V), there exists a basis of V with respect to which T has a reasonably simple matrix.
- **Definition**: The ***diagonal*** of a square matrix consists of the entries along the straight line from the upper left corner to the bottom right corner.
- **Definition**: A matrix is called ***upper triangular*** if all the entries below the diagonal equal 0.
- **Proposition**: Suppose T âˆˆ ğ“›(V) and (v1, ..., vn) is a basis of V. Then the following are equivalent:
    1. the matrix of T with respect to (v1, ..., vn) is upper triangular;
    2. Tvk âˆˆ span(v1, ..., vk) for each k = 1, ..., n;
    3. span(v1, ..., vk) is invariant under T for each k = 1, ..., n.
- **Theorem**: Suppose V is a complex vector space and T âˆˆ ğ“›(V). Then T has an upper-triangular matrix with respect to some basis of V.
- **Proposition**: Suppose T âˆˆ ğ“›(V) has an upper-triangular matrix with respect to some basis of V. Then T is invertible if and only if all the entries on the diagonal of that upper-triangular matrix are nonzero.
- **Proposition**: Suppose T âˆˆ ğ“›(V) has an upper-triangular matrix with respect to some basis of V. Then the eigenvalues of T consist precisely of the entries on the diagonal of that upper-triangular matrix.

## Diagonal Matrices

- **Definition**: A ***diagonal matrix*** is a square matrix that is 0 everywhere except possibly along the diagonal.
- **Proposition**: If T âˆˆ ğ“›(V) has dimV distinct eigenvalues, then T has a diagonal matrix with respect to some basis of V.
- **Proposition**: Suppose T âˆˆ ğ“›(V). Let â„·1, ..., â„·m denote the distinct eigenvalues of T. Then the following are equivalent:
    - T has a diagonal matrix with respect to some basis of V;
    - V has a basis consisting of eigenvalues of T;
    - there exist one-dimensional subspaces U1, ..., Un of V, each invariant under T, such that V = U1 âŠ• ... âŠ• Un;
    - V = null(T - â„·1I) âŠ• ... âŠ• null(T - â„·mI);
    - dimV = dim null(T - â„·1I) âŠ• ... âŠ• dim null(T - â„·mI);
- **Theorem**: Every operator on a finite-dimensional, nonzero, real vector space has an invariant subspace of dimension 1 or 2.
- **Theorem**: Every operator on an odd-dimensional real vector space has a eigenvalue.
