abstract: 'Traditional Reinforcement Learning (RL) algorithms either predict rewards

  with value functions or maximize them using policy search. We study an

  alternative: Upside-Down Reinforcement Learning (Upside-Down RL or UDRL), that

  solves RL problems primarily using supervised learning techniques. Many of its

  main principles are outlined in a companion report [34]. Here we present the

  first concrete implementation of UDRL and demonstrate its feasibility on

  certain episodic learning problems. Experimental results show that its

  performance can be surprisingly competitive with, and even exceed that of

  traditional baseline algorithms developed over decades of research.'
archiveprefix: arXiv
author: Rupesh Kumar Srivastava and Pranav Shyam and Filipe Mutz and Wojciech Jaśkowski
  and Jürgen Schmidhuber
eprint: 1912.02877v1
file: 1912.02877v1.pdf
files:
- /Users/Roy/Documents/papers/2019-training-agents-using-upside-down-reinforcement-learning.pdf
month: Dec
primaryclass: cs.LG
ref: 1912.02877v1
title: Training Agents using Upside-Down Reinforcement Learning
type: article
url: http://arxiv.org/abs/1912.02877v1
year: '2019'
