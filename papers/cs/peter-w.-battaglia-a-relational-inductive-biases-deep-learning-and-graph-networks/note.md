# Relational Inductive Biases, Deep Learning and Graph Networks



## Abstract

What the Graph Network supports:

* Relational Reasoning
* Combinatorial Generalization
  + Realized by Structured Representations and Computations
  + Achieve the ability to generalize beyond one's experiences



## Introduction

* A key signature of human intelligence is the capacity for combinatorial generalization.
* Which depends critically on our cognitive mechanisms for representing structure and reasoning about relations.
* We understand the world in compositional terms.
  1. Fit new knowledge into our existing structured representations;
  2. Adjust the structure itself to better accommodate the new and the old.
* The improved sample complexity afforded by structured approaches' strong inductive biases was very valuable in building artificial systems which exhibit combinatorial generalization.


* Modern deep learning methods often follow an "end-to-end" design philosophy which emphasizes minimal a priori representational and computational assumptions, and seeks to avoid explicit structure and "hand-engineering".
* But challenges in complex language and scene understanding, resoning about structured data, transferring learning beyond the training conditions, and learning from small amounts of experience demand combinatorial generalization.
* Structure and flexibility should be integrated to approach combinatorial generalization.
* By carrying strong relational inductive biases, there arise a class of models capable for performing computation over discrete entities and the relations between them.



### Summary

The design principles behind graph networks

* flexible representations
* configurable within-block structure 
* composable multi-block architectures



## Discussion



### Conclusion

