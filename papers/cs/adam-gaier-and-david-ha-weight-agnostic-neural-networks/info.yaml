abstract: 'Not all neural network architectures are created equal, some perform much

  better than others for certain tasks. But how important are the weight

  parameters of a neural network compared to its architecture? In this work, we

  question to what extent neural network architectures alone, without learning

  any weight parameters, can encode solutions for a given task. We propose a

  search method for neural network architectures that can already perform a task

  without any explicit weight training. To evaluate these networks, we populate

  the connections with a single shared weight parameter sampled from a uniform

  random distribution, and measure the expected performance. We demonstrate that

  our method can find minimal neural network architectures that can perform

  several reinforcement learning tasks without weight training. On a supervised

  learning domain, we find network architectures that achieve much higher than

  chance accuracy on MNIST using random weights. Interactive version of this

  paper at https://weightagnostic.github.io/'
archiveprefix: arXiv
author: Adam Gaier and David Ha
eprint: 1906.04358v1
file: 1906.04358v1.pdf
files:
- /Users/Roy/Documents/papers/weight-agnostic-neural-networks.pdf
month: Jun
primaryclass: cs.LG
ref: 1906.04358v1
title: Weight Agnostic Neural Networks
type: article
url: http://arxiv.org/abs/1906.04358v1
year: '2019'
