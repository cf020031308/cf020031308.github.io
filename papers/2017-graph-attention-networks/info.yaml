abstract: 'We present graph attention networks (GATs), novel neural network

  architectures that operate on graph-structured data, leveraging masked

  self-attentional layers to address the shortcomings of prior methods based on

  graph convolutions or their approximations. By stacking layers in which nodes

  are able to attend over their neighborhoods'' features, we enable (implicitly)

  specifying different weights to different nodes in a neighborhood, without

  requiring any kind of costly matrix operation (such as inversion) or depending

  on knowing the graph structure upfront. In this way, we address several key

  challenges of spectral-based graph neural networks simultaneously, and make our

  model readily applicable to inductive as well as transductive problems. Our GAT

  models have achieved or matched state-of-the-art results across four

  established transductive and inductive graph benchmarks: the Cora, Citeseer and

  Pubmed citation network datasets, as well as a protein-protein interaction

  dataset (wherein test graphs remain unseen during training).'
archiveprefix: arXiv
author: Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero
  and Pietro Liò and Yoshua Bengio
eprint: 1710.10903v3
file: 1710.10903v3.pdf
files:
- /Users/Roy/Documents/papers/2017-graph-attention-networks.pdf
month: Oct
primaryclass: stat.ML
ref: 1710.10903v3
title: Graph Attention Networks
type: article
url: http://arxiv.org/abs/1710.10903v3
year: '2017'
