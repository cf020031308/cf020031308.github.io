abstract: 'Inattentional blindness is the psychological phenomenon that causes one
  to

  miss things in plain sight. It is a consequence of the selective attention in

  perception that lets us remain focused on important parts of our world without

  distraction from irrelevant details. Motivated by selective attention, we study

  the properties of artificial agents that perceive the world through the lens of

  a self-attention bottleneck. By constraining access to only a small fraction of

  the visual input, we show that their policies are directly interpretable in

  pixel space. We find neuroevolution ideal for training self-attention

  architectures for vision-based reinforcement learning (RL) tasks, allowing us

  to incorporate modules that can include discrete, non-differentiable operations

  which are useful for our agent. We argue that self-attention has similar

  properties as indirect encoding, in the sense that large implicit weight

  matrices are generated from a small number of key-query parameters, thus

  enabling our agent to solve challenging vision based tasks with at least 1000x

  fewer parameters than existing methods. Since our agent attends to only task

  critical visual hints, they are able to generalize to environments where task

  irrelevant elements are modified while conventional methods fail. Videos of our

  results and source code available at https://attentionagent.github.io/'
archiveprefix: arXiv
author: Yujin Tang and Duong Nguyen and David Ha
doi: 10.1145/3377930.3389847
eprint: 2003.08165v2
file: 2003.08165v2.pdf
files:
- /Users/Roy/Documents/papers/2020-neuroevolution-of-self-interpretable-agents.pdf
month: Mar
primaryclass: cs.NE
ref: 2003.08165v2
title: Neuroevolution of Self-Interpretable Agents
type: article
url: http://arxiv.org/abs/2003.08165v2
year: '2020'
