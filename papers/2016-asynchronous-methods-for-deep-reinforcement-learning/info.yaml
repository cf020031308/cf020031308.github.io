abstract: 'We propose a conceptually simple and lightweight framework for deep

  reinforcement learning that uses asynchronous gradient descent for optimization

  of deep neural network controllers. We present asynchronous variants of four

  standard reinforcement learning algorithms and show that parallel

  actor-learners have a stabilizing effect on training allowing all four methods

  to successfully train neural network controllers. The best performing method,

  an asynchronous variant of actor-critic, surpasses the current state-of-the-art

  on the Atari domain while training for half the time on a single multi-core CPU

  instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds

  on a wide variety of continuous motor control problems as well as on a new task

  of navigating random 3D mazes using a visual input.'
archiveprefix: arXiv
author: Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves
  and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu
eprint: 1602.01783v2
file: 1602.01783v2.pdf
files:
- /Users/Roy/Documents/papers/2016-asynchronous-methods-for-deep-reinforcement-learning.pdf
month: Feb
note: ICML 2016
primaryclass: cs.LG
ref: 1602.01783v2
title: Asynchronous Methods for Deep Reinforcement Learning
type: article
url: http://arxiv.org/abs/1602.01783v2
year: '2016'
