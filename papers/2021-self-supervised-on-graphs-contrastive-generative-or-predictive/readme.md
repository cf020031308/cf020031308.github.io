# 图自监督学习：对比式、生成式与预测式

没看原文，仅梳理[他人笔记](https://mp.weixin.qq.com/s/3NkRcQsS0pzS4nm1JbxAjw)。

## 训练策略

1. 预训练和微调：无监督预训练好编码器后 append 个预测头进行有监督训练
2. 联合学习：编码器的无监督学习损失作为有监督预测损失的正则项
3. 无监督表征学习（URL）：无监督预训练得到表示后再监督学习从表示转到预测

## 对比式

### 数据增广

* 基于特征：对节点或边的特征加噪音，或随机交换两节点或边的特征
* 基于结构：随机增删边（甚至加节点），或基于链接预测补充边，或随机或按重要性（注意力或梯度变化）采样出子图

### 代理任务设计

按信息尺度（local, context, global）分为同尺度对比和跨尺度对比

* global-global
  * GraphCL, CSSL. 预测增广图是否来自原图
    * 正样本：原图的图表示与增广图的图表示
    * 负样本：原图的图表示与其它图增广图的图表示
* context-context
  * GCC. 预测采样子图是否来自相同图
    * 正样本：相同图的子图
    * 负样本：不同图的子图
* local-local
  * GRACE, GCA, GROC. 学习增广图中节点的表示
    * 正样本：不同增广图中相同节点的表示
    * intra-view 负样本：相同增广图中不同节点的表示
    * inter-view 负样本：不同增广图中不同节点的表示
* local-global
  * DGI. 大概是最大化增广图的图表示和原图节点表示的相似度，但负样本没看明白，怀疑综述有问题 TODO
* local-context
  * SUBG-CON. 以一堆锚节点为中心采样一堆子图
    * 正样本：节点表示和该节点为中心的子图的图表示
    * 负样本：节点表示和其它锚为中心的子图的图表示
* context-global
  * InfoGraph.
    * 正样本：图表示与节点表示
    * 负样本：图表示与增广图中节点表示

### 对比目标

最大化互信息，可以用不同方式最大化互信息的下界。

## 生成式

* 图自编码：节点特征加噪声后还原
* 图自回归

## 预测式

将图中的信息作为标签进行监督学习，如

* 节点属性预测，如度
* 基于上下文的预测，如节点距离
* 自训练，用聚类等得到的伪标签进行监督，并更新伪标签
* 基于领域知识的预测
