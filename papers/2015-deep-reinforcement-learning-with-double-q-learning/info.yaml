abstract: 'The popular Q-learning algorithm is known to overestimate action values
  under

  certain conditions. It was not previously known whether, in practice, such

  overestimations are common, whether they harm performance, and whether they can

  generally be prevented. In this paper, we answer all these questions

  affirmatively. In particular, we first show that the recent DQN algorithm,

  which combines Q-learning with a deep neural network, suffers from substantial

  overestimations in some games in the Atari 2600 domain. We then show that the

  idea behind the Double Q-learning algorithm, which was introduced in a tabular

  setting, can be generalized to work with large-scale function approximation. We

  propose a specific adaptation to the DQN algorithm and show that the resulting

  algorithm not only reduces the observed overestimations, as hypothesized, but

  that this also leads to much better performance on several games.'
archiveprefix: arXiv
author: Hado van Hasselt and Arthur Guez and David Silver
eprint: 1509.06461v3
file: 1509.06461v3.pdf
files:
- /Users/Roy/Documents/papers/2015-deep-reinforcement-learning-with-double-q-learning.pdf
month: Sep
primaryclass: cs.LG
ref: 1509.06461v3
title: Deep Reinforcement Learning with Double Q-learning
type: article
url: http://arxiv.org/abs/1509.06461v3
year: '2015'
