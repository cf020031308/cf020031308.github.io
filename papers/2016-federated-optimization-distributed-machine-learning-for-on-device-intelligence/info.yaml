abstract: 'We introduce a new and increasingly relevant setting for distributed

  optimization in machine learning, where the data defining the optimization are

  unevenly distributed over an extremely large number of nodes. The goal is to

  train a high-quality centralized model. We refer to this setting as Federated

  Optimization. In this setting, communication efficiency is of the utmost

  importance and minimizing the number of rounds of communication is the

  principal goal.

  A motivating example arises when we keep the training data locally on users''

  mobile devices instead of logging it to a data center for training. In

  federated optimziation, the devices are used as compute nodes performing

  computation on their local data in order to update a global model. We suppose

  that we have extremely large number of devices in the network --- as many as

  the number of users of a given service, each of which has only a tiny fraction

  of the total data available. In particular, we expect the number of data points

  available locally to be much smaller than the number of devices. Additionally,

  since different users generate data with different patterns, it is reasonable

  to assume that no device has a representative sample of the overall

  distribution.

  We show that existing algorithms are not suitable for this setting, and

  propose a new algorithm which shows encouraging experimental results for sparse

  convex problems. This work also sets a path for future research needed in the

  context of \federated optimization.'
archiveprefix: arXiv
author: Jakub Konečný and H. Brendan McMahan and Daniel Ramage and Peter Richtárik
eprint: 1610.02527v1
file: 1610.02527v1.pdf
files:
- /Users/Roy/Documents/papers/2016-federated-optimization-distributed-machine-learning-for-on-device-intelligence.pdf
month: Oct
primaryclass: cs.LG
ref: 1610.02527v1
title: 'Federated Optimization: Distributed Machine Learning for On-Device

  Intelligence'
type: article
url: http://arxiv.org/abs/1610.02527v1
year: '2016'
