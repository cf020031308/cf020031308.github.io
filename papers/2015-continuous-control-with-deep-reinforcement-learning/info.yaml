abstract: 'We adapt the ideas underlying the success of Deep Q-Learning to the

  continuous action domain. We present an actor-critic, model-free algorithm

  based on the deterministic policy gradient that can operate over continuous

  action spaces. Using the same learning algorithm, network architecture and

  hyper-parameters, our algorithm robustly solves more than 20 simulated physics

  tasks, including classic problems such as cartpole swing-up, dexterous

  manipulation, legged locomotion and car driving. Our algorithm is able to find

  policies whose performance is competitive with those found by a planning

  algorithm with full access to the dynamics of the domain and its derivatives.

  We further demonstrate that for many of the tasks the algorithm can learn

  policies end-to-end: directly from raw pixel inputs.'
archiveprefix: arXiv
author: Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas
  Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra
eprint: 1509.02971v6
file: 1509.02971v6.pdf
files:
- /Users/Roy/Documents/papers/2015-continuous-control-with-deep-reinforcement-learning.pdf
month: Sep
primaryclass: cs.LG
ref: 1509.02971v6
title: Continuous control with deep reinforcement learning
type: article
url: http://arxiv.org/abs/1509.02971v6
year: '2015'
