abstract: 'In this paper we argue for the fundamental importance of the value

  distribution: the distribution of the random return received by a reinforcement

  learning agent. This is in contrast to the common approach to reinforcement

  learning which models the expectation of this return, or value. Although there

  is an established body of literature studying the value distribution, thus far

  it has always been used for a specific purpose such as implementing risk-aware

  behaviour. We begin with theoretical results in both the policy evaluation and

  control settings, exposing a significant distributional instability in the

  latter. We then use the distributional perspective to design a new algorithm

  which applies Bellman''s equation to the learning of approximate value

  distributions. We evaluate our algorithm using the suite of games from the

  Arcade Learning Environment. We obtain both state-of-the-art results and

  anecdotal evidence demonstrating the importance of the value distribution in

  approximate reinforcement learning. Finally, we combine theoretical and

  empirical evidence to highlight the ways in which the value distribution

  impacts learning in the approximate setting.'
archiveprefix: arXiv
author: Marc G. Bellemare and Will Dabney and RÃ©mi Munos
eprint: 1707.06887v1
file: 1707.06887v1.pdf
files:
- /Users/Roy/Documents/papers/2017-a-distributional-perspective-on-reinforcement-learning.pdf
month: Jul
primaryclass: cs.LG
ref: 1707.06887v1
title: A Distributional Perspective on Reinforcement Learning
type: article
url: http://arxiv.org/abs/1707.06887v1
year: '2017'
