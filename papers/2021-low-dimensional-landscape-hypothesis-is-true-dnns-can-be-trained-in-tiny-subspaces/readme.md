# 深度神经网络的低维猜想

读硕士时隔壁院邀请黄晓霖老师来做报告并分享了这项工作，该工作今年发表在了领域最牛期刊TRAMI上（我这看的还是2021年版本）。
最近调研相关问题想起这篇因此重新阅读了其论文和一些代码。

方法大概是说，在深度神经网络DNN训练的训练过程中，各轮参数更新组成的矩阵可用主成分分析降维，得到一个低维子空间，在这个低维子空间中（继续）训练可达到标准训练的效果。

问题是实验里这条轨迹取得太长，损失曲线都接近收敛了，然后从这开始转到低维空间中继续优化，于是说DNN可以在低维空间中优化我觉得论点不是很立得住。

另外想看看这项工作里，导出子空间后，是怎么在子空间限制下做优化的。
翻看代码后发现是正常反向传播后，将梯度投影到子空间中。
