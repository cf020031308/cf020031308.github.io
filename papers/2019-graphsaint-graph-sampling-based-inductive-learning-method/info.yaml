abstract: 'Graph Convolutional Networks (GCNs) are powerful models for learning

  representations of attributed graphs. To scale GCNs to large graphs,

  state-of-the-art methods use various layer sampling techniques to alleviate the

  "neighbor explosion" problem during minibatch training. We propose GraphSAINT,

  a graph sampling based inductive learning method that improves training

  efficiency and accuracy in a fundamentally different way. By changing

  perspective, GraphSAINT constructs minibatches by sampling the training graph,

  rather than the nodes or edges across GCN layers. Each iteration, a complete

  GCN is built from the properly sampled subgraph. Thus, we ensure fixed number

  of well-connected nodes in all layers. We further propose normalization

  technique to eliminate bias, and sampling algorithms for variance reduction.

  Importantly, we can decouple the sampling from the forward and backward

  propagation, and extend GraphSAINT with many architecture variants (e.g., graph

  attention, jumping connection). GraphSAINT demonstrates superior performance in

  both accuracy and training time on five large graphs, and achieves new

  state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).'
archiveprefix: arXiv
author: Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan
  and Viktor Prasanna
eprint: 1907.04931v4
file: 1907.04931v4.pdf
files:
- /Users/Roy/Documents/papers/2019-graphsaint-graph-sampling-based-inductive-learning-method.pdf
month: Jul
primaryclass: cs.LG
ref: 1907.04931v4
title: 'GraphSAINT: Graph Sampling Based Inductive Learning Method'
type: article
url: http://arxiv.org/abs/1907.04931v4
year: '2019'
