# å­¦ä¹ å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­çš„å°é—®é¢˜


## å‚è€ƒä¹¦ç›®

* é‚¹ä¼Ÿã€Šå¼ºåŒ–å­¦ä¹ ã€‹
* å‘¨å¿—åã€Šæœºå™¨å­¦ä¹ ã€‹ä¸­å¼ºåŒ–å­¦ä¹ ç« èŠ‚
* é‚±é”¡é¹ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ä¸­çš„å¼ºåŒ–å­¦ä¹ ç« èŠ‚
* *Reinforcement Learning: An Introduction* by Richard Sutton


## ğœ–è´ªå¿ƒç­–ç•¥æ”¹è¿›åŸå§‹ç­–ç•¥çš„æ¡ä»¶


é‚¹ä¼Ÿã€Šè’™ç‰¹å¡ç½—æ§åˆ¶ã€‹ä¸€ç« æœ‰è¯æ˜

> â€œğœ–è´ªå¿ƒç­–ç•¥å¯ä»¥æ”¹è¿›ä»»æ„ä¸€ä¸ªç»™å®šçš„ç­–ç•¥ã€‚â€

ç»†çœ‹è¿‡ç¨‹ä¼¼æœ‰é—®é¢˜ï¼Œå•çœ‹è®ºç‚¹ä¹Ÿç«™ä¸ä½è„šï¼šè€ƒè™‘æç«¯æƒ…å†µï¼Œå‡å¦‚ä¸€ä¸ªç­–ç•¥æœ¬èº«å·²æ˜¯æœ€ä¼˜çš„ï¼Œé‚£ä¹ˆåŠ å…¥éšæœºæ¢ç´¢åŠ¨ä½œåå›æŠ¥åªå¯èƒ½å‡å°‘ï¼Œé‘è®ºæ”¹è¿›ã€‚


æŸ¥åˆ°Richard Suttonçš„ä¹¦é‡Œè¯´å¾—æ¸…æ¥š

> any Îµ-greedy policy with respect to $q_\pi$ is an improvement over any Îµ-soft policy Ï€ is assured by the policy improvement theorem


è¯´æ˜äº†ğœ–è´ªå¿ƒæ‰€æ”¹è¿›çš„åŸå§‹ç­–ç•¥å¹¶éä»»æ„ç­–ç•¥ï¼Œè€Œæ˜¯Îµ-softçš„ç­–ç•¥ã€‚  
è€ŒÎµ-soft policyçš„å®šä¹‰åˆ™è¦æ±‚å¯¹æ¯ä¸ªåŠ¨ä½œ$a \in A$éƒ½æœ‰$\pi(a|s) \ge \frac{\epsilon}{|A|}$ï¼Œå¦‚æ­¤æ‰æœ‰
$$ \max\limits_{a} q_\pi(s, a) \ge \sum\limits_{a} [\pi(a|s) - \frac{\epsilon}{|A|}] q_\pi(s, a) $$
ï¼ˆå¦åˆ™å¤§äºç­‰äºä¸æˆç«‹ï¼‰è¿›è€Œä¿è¯æ”¹è¿›æ•ˆæœã€‚


ç›´è§‚åœ°ç†è§£ï¼Œä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥æ··å…¥æ¦‚ç‡ä¸ºÎµçš„æ¢ç´¢åŠ¨ä½œåå°±æ˜¯Îµ-softçš„ã€‚æ‰€ä»¥å¦‚æœç§°è¿™æ ·çš„ç­–ç•¥ä¸ºâ€œÎµç­–ç•¥â€çš„è¯ï¼Œæ›´ä¸¥è°¨çš„è¯´æ³•åº”è¯¥æ˜¯â€œğœ–è´ªå¿ƒç­–ç•¥å¯ä»¥æ”¹è¿›ä»»æ„ğœ–ç­–ç•¥â€ã€‚

## Q-Learningä¸­çš„Maximization Bias


Double Q-Learningè¯´ä¼ ç»Ÿçš„Q-Learningå’ŒDQNéƒ½ä¼šå› ä¸ºä»¥ä¸‹æ›´æ–°å¼ä¸­çš„maxè€Œæ™®éè¿‡é«˜ä¼°è®¡Qï¼Œå­˜åœ¨è¿‡ä¼˜åŒ–é—®é¢˜
$$Q(s, a) \mathrel{+}= \alpha [r + \gamma \max\limits_{a' \in A} Q(s', a') - Q(s, a)]$$
ä½†ä¸ºä½•maxæ“ä½œä¼šå¯¼è‡´Maximization Biasï¼Ÿ


å‡è®¾åŠ¨ä½œå€¼å‡½æ•°å·²è¢«ä¼˜åŒ–åˆ°äº†ä¸€å®šç¨‹åº¦ï¼Œè®¾ä¸º$Q + \epsilon$ï¼Œå…¶ä¸­Qæ˜¯çœŸå®çš„åŠ¨ä½œå€¼å‡½æ•°ï¼Œ$\epsilon \sim N(0, 1)$ï¼Œåˆ™
$$\mathbb E_\epsilon[ \max (Q + \epsilon) ] \ge \mathbb E_\epsilon[ Q^* + \epsilon ] = Q^* + \mathbb E\epsilon = Q^* $$
å³å–maxååœ¨æœŸæœ›ä¸Šä¼šæ¯”çœŸå®çš„$ Q^* = \max Q $æ›´å¤§ï¼Œæ‰€ä»¥ä¼°è®¡ä¼šåå¤§ã€‚

## REINFORCEç®—æ³•ä¸­çš„$\gamma^t$æ¥æº


REINFORCEç®—æ³•çš„ç­–ç•¥å‡½æ•°å‚æ•°æ›´æ–°æ–¹å¼ä¸º
$$\theta \mathrel{+}= \alpha \gamma^t G(\tau_{t:T}) \frac{\partial{}}{\partial{\theta}} \log \pi_\theta(a_t|s_t)$$

æ›´æ–°é¡¹ä¸ºç­–ç•¥æ¢¯åº¦ä¸­tæ—¶åˆ»ä¹‹åçš„éƒ¨åˆ†ï¼Œéšå«çš„æ„æ€æ˜¯tæ—¶åˆ»ä¹‹å‰çš„éƒ¨åˆ†æœŸæœ›ä¸º0ï¼š
$$\mathbb E \sum\limits_{t=0}^{T-1} \frac{\partial{}}{\partial{\theta}} \log \pi_\theta (a_t | s_t) G(\tau_{0:t-1}) = 0$$
å¦‚ä½•è¯æ˜ï¼Ÿ


è¯¥ç®—æ³•åé¢ä¼šå­¦ä¹ ä¸€ä¸ªå¸¦åŸºçº¿çš„REINFORCEç®—æ³•ï¼Œè¯æ˜äº†ä¸ºç­–ç•¥æ¢¯åº¦å¼•å…¥ä¸€ä¸ªå’Œ$a_t$æ— å…³çš„åŸºå‡†å‡½æ•°$b(s_t)$åæœŸæœ›ä¸å˜ï¼Œå³ï¼ˆè¿‡ç¨‹ä¹¦ä¸Šæœ‰ï¼‰ï¼š
$$\mathbb E b(s_t) \frac{\partial{}}{\partial{\theta}} \log \pi_\theta (a_t | s_t) = 0$$
å› ä¸º$G(\tau_{0:t-1})$ä¹Ÿè·Ÿ$a_t$æ— å…³ï¼Œæ‰€ä»¥ä»£å…¥$b(s_t) = G(\tau_{0:t-1})$åŒç†å¯è¯ã€‚


## å¸¦åŸºçº¿çš„Reinforceç®—æ³•ä¸­$\alpha = 1$çš„è¯æ˜


é‚±é”¡é¹ä¹¦ä¸­æä¾›äº†ä½¿ç”¨åŸºçº¿å¯å‡å°REINFORCEç®—æ³•æ–¹å·®æœ‰ä¸»è¦è¯æ˜ï¼ˆè¯¦è§æˆ‘çš„ç¬”è®°[æ–¹å·®å‰Šå‡](/wiki/variance-reduction.md)ï¼‰ï¼Œä½†å–Vä¸ºåŸºçº¿åå°±æŠŠå› å­$\alpha$å»æ‰äº†ï¼Œä¸ºä½•$\alpha = 1$ï¼Ÿ


å› ä¸º$V = \mathbb E_{a} Q$ï¼Œæ‰€ä»¥
$$\begin{aligned}
\text{cov}(Q, V) &= \mathbb E_{a, s}(QV) - \mathbb E_{a, s} Q \cdot \mathbb E_{s} V \\\\
&= \mathbb E_{s} V^2 - \mathbb E_{s}^2V \\\\
&= \mathbb DV
\end{aligned}$$
æ‰€ä»¥$\alpha = \frac{\text{cov}(Q, V)}{\mathbb DV} = 1$
